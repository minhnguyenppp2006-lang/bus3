import streamlit as st
import openrouteservice
import google.generativeai as genai
from geopy.geocoders import Nominatim
import speech_recognition as sr
from gtts import gTTS
from streamlit_mic_recorder import mic_recorder
import io
import tempfile
import os

# --- C·∫§U H√åNH TRANG ---
st.set_page_config(page_title="Bus Assistant (Free & Geopy)", page_icon="üöå", layout="wide")

# --- L·∫§Y API KEY T·ª™ SECRETS HO·∫∂C NH·∫¨P TAY ---
try:
    # ∆Øu ti√™n l·∫•y t·ª´ secrets
    ORS_API_KEY = st.secrets.get("ORS_API_KEY", "")
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
except:
    # N·∫øu ch·∫°y local ch∆∞a c√≥ file secrets
    ORS_API_KEY = ""
    GEMINI_API_KEY = ""

# --- SIDEBAR C·∫§U H√åNH ---
with st.sidebar:
    st.header("‚öôÔ∏è C·∫•u h√¨nh")
    
    # N·∫øu ch∆∞a c√≥ Key trong secrets th√¨ hi·ªán √¥ nh·∫≠p
    if not ORS_API_KEY:
        ORS_API_KEY = st.text_input("Nh·∫≠p OpenRouteService Key", type="password")
        st.caption("[L·∫•y Key mi·ªÖn ph√≠ t·∫°i ƒë√¢y](https://openrouteservice.org/dev/#/home)")
        
    if not GEMINI_API_KEY:
        GEMINI_API_KEY = st.text_input("Nh·∫≠p Gemini API Key", type="password")
        st.caption("[L·∫•y Key mi·ªÖn ph√≠ t·∫°i ƒë√¢y](https://aistudio.google.com/)")

    auto_speak = st.toggle("üîä ƒê·ªçc to c√¢u tr·∫£ l·ªùi", value=True)
    st.info("Phi√™n b·∫£n s·ª≠ d·ª•ng Geopy ƒë·ªÉ ƒë·ªãnh v·ªã t·ªët h∆°n.")

# --- H√ÄM X·ª¨ L√ù ƒê·ªäA L√ù (GEOPY + ORS) ---

def get_coordinates(address):
    """
    D√πng Geopy (Nominatim) ƒë·ªÉ t√¨m t·ªça ƒë·ªô t·ª´ ƒë·ªãa ch·ªâ.
    Kh√¥ng c·∫ßn API Key, t√¨m ti·∫øng Vi·ªát t·ªët.
    """
    # User_agent l√† b·∫Øt bu·ªôc ƒë·ªÉ ƒë·ªãnh danh ·ª©ng d·ª•ng c·ªßa b·∫°n
    geolocator = Nominatim(user_agent="vietnam_bus_assistant_app_v1")
    
    try:
        # Th√™m 'Vi·ªát Nam' ƒë·ªÉ t√¨m ch√≠nh x√°c h∆°n n·∫øu ng∆∞·ªùi d√πng qu√™n nh·∫≠p
        search_query = address
        if "vi·ªát nam" not in address.lower():
            search_query += ", Vi·ªát Nam"
            
        location = geolocator.geocode(search_query, timeout=10)
        
        if location:
            # L∆∞u √Ω: ORS c·∫ßn [Longitude, Latitude] (Kinh ƒë·ªô tr∆∞·ªõc)
            # Geopy tr·∫£ v·ªÅ (Latitude, Longitude) (Vƒ© ƒë·ªô tr∆∞·ªõc) -> C·∫ßn ƒë·∫£o ng∆∞·ª£c
            return [location.longitude, location.latitude], location.address
        return None, None
    except Exception as e:
        return None, str(e)

def get_route_ors(start_addr, end_addr, client):
    """T√¨m ƒë∆∞·ªùng ƒëi b·ªô/xe k·∫øt h·ª£p Geopy v√† OpenRouteService"""
    
    # 1. ƒê·ªãnh v·ªã (Geocoding)
    start_coords, start_full = get_coordinates(start_addr)
    end_coords, end_full = get_coordinates(end_addr)
    
    # X·ª≠ l√Ω l·ªói ƒë·ªãnh v·ªã
    if not start_coords:
        return None, f"Kh√¥ng t√¨m th·∫•y ƒëi·ªÉm ƒëi: '{start_addr}'. H√£y th·ª≠ nh·∫≠p c·ª• th·ªÉ h∆°n (VD: S·ªë nh√†, Ph∆∞·ªùng, Qu·∫≠n)."
    if not end_coords:
        return None, f"Kh√¥ng t√¨m th·∫•y ƒëi·ªÉm ƒë·∫øn: '{end_addr}'. H√£y th·ª≠ nh·∫≠p c·ª• th·ªÉ h∆°n."

    try:
        # 2. V·∫Ω ƒë∆∞·ªùng (Routing)
        # profile='foot-walking' (ƒëi b·ªô) ho·∫∑c 'driving-car' (xe h∆°i)
        route = client.directions(
            coordinates=[start_coords, end_coords],
            profile='foot-walking', 
            format='geojson',
            language='vi'
        )
        
        # 3. Tr√≠ch xu·∫•t d·ªØ li·ªáu
        summary = route['features'][0]['properties']['segments'][0]
        distance_km = round(summary['distance'] / 1000, 2)
        duration_min = round(summary['duration'] / 60)
        
        # L·∫•y c√°c b∆∞·ªõc ƒëi
        steps_list = []
        for step in summary['steps']:
            steps_list.append(f"- {step['instruction']} ({step['distance']}m)")
            
        steps_str = "\n".join(steps_list)

        return {
            "start_original": start_addr,
            "end_original": end_addr,
            "start_found": start_full,
            "end_found": end_full,
            "distance": f"{distance_km} km",
            "duration": f"{duration_min} ph√∫t ƒëi b·ªô",
            "steps": steps_str
        }, None

    except Exception as e:
        return None, f"L·ªói t√¨m ƒë∆∞·ªùng ORS: {str(e)}"

# --- H√ÄM X·ª¨ L√ù √ÇM THANH ---
def text_to_speech(text):
    try:
        if not text: return None
        tts = gTTS(text=text, lang='vi')
        fp = io.BytesIO()
        tts.write_to_fp(fp)
        return fp
    except: return None

def process_audio_input(audio_bytes):
    r = sr.Recognizer()
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
            tmp.write(audio_bytes)
            tmp_name = tmp.name
        with sr.AudioFile(tmp_name) as source:
            audio_data = r.record(source)
            text = r.recognize_google(audio_data, language="vi-VN")
        os.remove(tmp_name)
        return text
    except: return None

# --- GIAO DI·ªÜN CH√çNH ---

st.title("üöå Bus Assistant AI (Geopy Version)")
st.caption("ƒê·ªãnh v·ªã b·∫±ng Nominatim - T√¨m ƒë∆∞·ªùng b·∫±ng OpenRouteService - T∆∞ v·∫•n b·∫±ng Gemini")

# Ki·ªÉm tra Key
if not ORS_API_KEY or not GEMINI_API_KEY:
    st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p ƒë·ªß API Key ·ªü thanh b√™n tr√°i (Sidebar) ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
    st.stop()

# Kh·ªüi t·∫°o Client
ors_client = openrouteservice.Client(key=ORS_API_KEY)
genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-flash-latest') # D√πng b·∫£n Flash cho nhanh v√† Free

# Chia c·ªôt
col1, col2 = st.columns([1, 1])

# --- C·ªòT TR√ÅI: T√åM KI·∫æM ---
with col1:
    st.subheader("üìç L·ªô Tr√¨nh")
    start_input = st.text_input("ƒêi·ªÉm ƒëi", placeholder="VD: B·∫øn xe Mi·ªÅn T√¢y")
    end_input = st.text_input("ƒêi·ªÉm ƒë·∫øn", placeholder="VD: ƒê·∫°i h·ªçc Qu·ªëc gia TPHCM")
    
    if st.button("T√¨m ƒë∆∞·ªùng üöÄ", type="primary"):
        if start_input and end_input:
            with st.spinner("ƒêang ƒë·ªãnh v·ªã v√† t√≠nh to√°n..."):
                data, error = get_route_ors(start_input, end_input, ors_client)
                
                if error:
                    st.error(error)
                else:
                    # Hi·ªÉn th·ªã k·∫øt qu·∫£
                    st.success("ƒê√£ t√¨m th·∫•y l·ªô tr√¨nh!")
                    st.write(f"**T·ª´:** {data['start_found']}")
                    st.write(f"**ƒê·∫øn:** {data['end_found']}")
                    
                    m1, m2 = st.columns(2)
                    m1.metric("Kho·∫£ng c√°ch", data['distance'])
                    m2.metric("Th·ªùi gian ƒëi b·ªô", data['duration'])
                    
                    # L∆∞u context v√†o session
                    context_str = f"""
                    Th√¥ng tin chuy·∫øn ƒëi:
                    - ƒêi·ªÉm ƒëi: {data['start_found']}
                    - ƒêi·ªÉm ƒë·∫øn: {data['end_found']}
                    - Kho·∫£ng c√°ch th·ª±c t·∫ø: {data['distance']}
                    - Th·ªùi gian n·∫øu ƒëi b·ªô: {data['duration']}
                    """
                    st.session_state['route_context'] = context_str
                    
                    with st.expander("Chi ti·∫øt ƒë∆∞·ªùng ƒëi b·ªô (Tham kh·∫£o)"):
                        st.text(data['steps'])
        else:
            st.toast("Vui l√≤ng nh·∫≠p c·∫£ ƒëi·ªÉm ƒëi v√† ƒë·∫øn!")

# --- C·ªòT PH·∫¢I: AI CHAT ---
with col2:
    st.subheader("üí¨ Tr·ª£ L√Ω ·∫¢o")
    
    chat_container = st.container(height=400)
    
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "Ch√†o b·∫°n! H√£y t√¨m l·ªô tr√¨nh b√™n tr√°i, sau ƒë√≥ t√¥i s·∫Ω g·ª£i √Ω tuy·∫øn xe bu√Ωt ph√π h·ª£p."}]
        
    for msg in st.session_state.messages:
        chat_container.chat_message(msg["role"]).write(msg["content"])
        
    # Input khu v·ª±c
    c_input, c_mic = st.columns([5, 1])
    user_text = c_input.chat_input("H·ªèi t√¥i v·ªÅ tuy·∫øn xe bu√Ωt...")
    with c_mic:
        st.write("") # Spacer
        st.write("")
        mic_data = mic_recorder(start_prompt="üé§", stop_prompt="‚èπÔ∏è", key='mic', use_container_width=True)
    
    # X·ª≠ l√Ω input
    final_prompt = user_text
    
    # Logic x·ª≠ l√Ω Mic
    if mic_data and ('last_mic_id' not in st.session_state or st.session_state.last_mic_id != mic_data['id']):
        st.session_state.last_mic_id = mic_data['id']
        text_from_audio = process_audio_input(mic_data['audio']['bytes'])
        if text_from_audio:
            final_prompt = text_from_audio
    
    if final_prompt:
        # Hi·ªÉn th·ªã User
        st.session_state.messages.append({"role": "user", "content": final_prompt})
        chat_container.chat_message("user").write(final_prompt)
        
        # G·ªçi AI
        try:
            current_context = st.session_state.get('route_context', 'Ng∆∞·ªùi d√πng ch∆∞a nh·∫≠p l·ªô tr√¨nh c·ª• th·ªÉ.')
            
            # Prompt k·ªπ thu·∫≠t (Prompt Engineering)
            system_prompt = f"""
            B·∫°n l√† tr·ª£ l√Ω giao th√¥ng c√¥ng c·ªông th√¥ng minh t·∫°i Vi·ªát Nam.
            D·ªØ li·ªáu h·ªá th·ªëng cung c·∫•p: {current_context}
            
            Y√äU C·∫¶U:
            1. D·ª±a v√†o ƒëi·ªÉm ƒëi v√† ƒë·∫øn trong d·ªØ li·ªáu (n·∫øu c√≥), h√£y d√πng ki·∫øn th·ª©c c√≥ s·∫µn c·ªßa b·∫°n ƒë·ªÉ ƒê·ªÄ XU·∫§T C√ÅC TUY·∫æN XE BU√ùT (Bus numbers) ph√π h·ª£p nh·∫•t.
            2. N·∫øu kho·∫£ng c√°ch > 10km, h√£y nh·∫Øc ng∆∞·ªùi d√πng chu·∫©n b·ªã l·ªô tr√¨nh d√†i.
            3. Tr·∫£ l·ªùi c√¢u h·ªèi: "{final_prompt}"
            4. Phong c√°ch: Ng·∫Øn g·ªçn, h·ªØu √≠ch, ti·∫øng Vi·ªát t·ª± nhi√™n.
            """
            
            response = model.generate_content(system_prompt).text
            
            # Hi·ªÉn th·ªã AI
            st.session_state.messages.append({"role": "assistant", "content": response})
            chat_container.chat_message("assistant").write(response)
            
            # ƒê·ªçc to
            if auto_speak:
                audio_file = text_to_speech(response)
                if audio_file:
                    st.audio(audio_file, format='audio/mp3', start_time=0)
                    
        except Exception as e:
            st.error(f"AI Error: {e}")